---
title: "Module notebook"
output: html_notebook
---
```{r slider}
library(manipulate)
outcomes <- c(1, 2, 3, 4, 5, 6)
manipulate(hist(sample(outcomes, n, replace = TRUE), breaks = c(0.5, 1.5, 2.5, 
    3.5, 4.5, 5.5, 6.5), probability = TRUE, main = paste("Histogram of Outcomes of ", 
    n, " Die Rolls", sep = ""), xlab = "roll", ylab = "probability"), n = slider(0, 
    10000, initial = 100, step = 100))
```
Write a function to simulate rolling a die where you pass the number of rolls as an argument. Then, use your function to simulate rolling two dice 1000 times and take the sum of the rolls. Plot a histogram of those results.
```{r}
nrolls <- 1000
roll <- function(x) {
    sample(1:6, x, replace = TRUE)
}
two_dice <- roll(nrolls) + roll(nrolls)
hist(two_dice, breaks = c(1.5:12.5), probability = TRUE, main = "Rolling Two Dice", 
    xlab = "sum of rolls", ylab = "probability")
```

```{r}
outcomes <- c("heads", "tails")
prob <- c(1/2, 1/2)
barplot(prob, ylim = c(0, 0.6), names.arg = outcomes, space = 0.1, xlab = "outcome", 
    ylab = "Pr(X = outcome)", main = "Probability Mass Function")
```

```{r} 
cumprob<-cumsum(prob)
barplot(cumprob, names.arg=outcomes, space = 0.1, xlab="outcome", ylab = "Cumulative Pr(X)", main="Cumulative Probability")
```
I don't understand why this gives this graph. Is heads and tails meant to be round one, round two? Or only head, head or tail?


```{r}
outcomes <- c(1, 2, 3, 4, 5, 6)
prob <- c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)
barplot(prob, ylim = c(0, 0.5), names.arg = outcomes, space = 0.1, xlab = "outcome", 
    ylab = "Pr(X = outcome)", main = "Probability Mass Function")
```

```{r}
cumprob <- cumsum(prob)
barplot(cumprob, names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)", 
    main = "Cumulative Probability")
```

Regression on percentage or proportion follows beta distribution 
```{r}
library(ggplot2)
a <- 2
b <- 1
K <- 2
x <- seq(from = 0, to = 1, by = 0.025)
fx <- K * x^(a - 1) * (1 - x)^(b - 1)
lower_x <- seq(from = -0.25, to = 0, by = 0.025)  # add some values of x less than zero
upper_x <- seq(from = 1, to = 1.25, by = 0.025)  # add some values of x greater than one
lower_fx <- rep(0, 11)  # add fx=0 values to x<0
upper_fx <- rep(0, 11)  # add fx=0 values to x>1
x <- c(lower_x, x, upper_x)  # paste xs together
fx <- c(lower_fx, fx, upper_fx)  # paste fxs together
d <- as.data.frame(cbind(x, fx))
p <- ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") + geom_line()
p
```
```{r}
library(manipulate)
manipulate(ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") + 
    geom_line() + geom_polygon(data = data.frame(xvals = c(0, n, n, 0), fxvals = c(0, 
    K * n^(a - 1) * (1 - n)^(b - 1), 0, 0)), aes(x = xvals, y = fxvals)) + ggtitle(paste("Area Under Function = ", 
    0.5 * n * K * n^(a - 1) * (1 - n)^(b - 1), sep = " ")), n = slider(0, 1, 
    initial = 0.5, step = 0.01))
```
```{r}
x <- seq(from = 0, to = 1, by = 0.005)
prob <- 0.5 * x * K * x^(a - 1) * (1 - x)^(b - 1)
barplot(prob, names.arg = x, space = 0, main = "Cumulative Probability", xlab = "x", 
    ylab = "Pr(X ≤ x)")
```

What is the chance of getting a “1” on each of six consecutive rolls of a die? What about of getting exactly three “1”s? What is the expected number of “1”s to occur in six consecutive rolls?


```{r}
n <- 6  # number of trials
k <- 6  # number of successes
p <- 1/6
prob <- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n - 
    k)
prob
```
```{r}
k <- 3  # number of successes
prob <- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n - 
    k)
prob
```

As for other distributions, R has a built in density function, the dbinom() function, that you can use to solve for the probability of a given outcome, i.e., Pr (X=x)
```{r}
dbinom(x = k, size = n, prob = p)
```

We can also use the built in function pbinom() to return the value of the cumulative distribution function for the binomial distribution, i.e., the probability of observing up to and including a given number of successes in n
trials.

So, for example, the chances of observing exactly 0, 1, 2, 3, … 6 rolls of “1” on 6 rolls of a die are…
```{r}
probset <- dbinom(x = 0:6, size = 6, prob = 1/6)  # x is number of successes, size is number of trials
barplot(probset, names.arg = 0:6, space = 0, xlab = "outcome", ylab = "Pr(X = outcome)", 
    main = "Probability Mass Function")

```

```{r}
cumprob = cumsum(probset)
barplot(cumprob, names.arg = 0:6, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)", 
    main = "Cumulative Probability")
```

sum(probset)  # equals 1, as it should
## [1] 1
The chance of observing exactly 3 rolls of “1” is…

dbinom(x = 3, size = 6, prob = 1/6)
## [1] 0.05358368
And the chance of observing up to and including 3 rolls of “1” is…

pbinom(q = 3, size = 6, prob = 1/6)  # note the name of the argument is q not x
## [1] 0.991298
… which can also be calculated by summing the relevant individual outcome probabilities…

sum(dbinom(x = 0:3, size = 6, prob = 1/6))  # this sums the probabilities of 0, 1, 2, and 3 successes
## [1] 0.991298
The probability of observing more than 3 rolls of “1” is given as…

1 - pnbinom(q = 3, size = 6, prob = 1/6)
## [1] 0.9988642
or, alterntatively…

pnbinom(q = 3, size = 6, prob = 1/6, lower.tail = FALSE)
## [1] 0.9988642
The probability of observing 3 or more rolls of “1” is…

1 - pbinom(q = 2, size = 6, prob = 1/6)  # note here that the q argument is '2'
## [1] 0.06228567
or, alternatively…

pbinom(q = 2, size = 6, prob = 1/6, lower.tail = FALSE)
## [1] 0.06228567

```{r}
a <- 4
b <- 8
x <- seq(from = a - (b - a), to = b + (b - a), by = 0.01)
fx <- dunif(x, min = a, max = b)  # dunif() evaluates the density at each x
plot(x, fx, type = "l", xlab = "x", ylab = "f(x)", main = "Probability Density Function")
```

CHALLENGE:

Simulate a sample of 10000 random numbers from a uniform distribution in the interval between a = 6 and b = 8. Calculate the mean and variance of this simulated sample and compare it to the expectation for these parameters.
```{r}
#no idea what im doing here

sim<-runif(10000, 6, 8)
a<-6
b<-8
uni<-dunif(sim, min=6, max=8)
mean(uni)
sd (uni)
```
```{r}
mu <- 4
sigma <- 1.5
curve(dnorm(x, mu, sigma), mu - 4 * sigma, mu + 4 * sigma, main = "Normal Curve", 
    xlab = "x", ylab = "f(x)")
```
> mean(sim)
[1] 7.002211
> sd(sim)
[1] 0.5743454
> hist(sim)




```{r}
manipulate(plot(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000), 
    dnorm(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000), 
        mean = mu, sd = sigma), type = "l", xlim = c(mu - 4 * sigma, mu + 4 * 
        sigma), xlab = "x", ylab = "f(x)", main = "Normal Probability Density Function") + 
    polygon(rbind(c(mu - nsigma * sigma, 0), cbind(seq(from = (mu - nsigma * 
        sigma), to = (mu + nsigma * sigma), length.out = 1000), dnorm(seq(from = (mu - 
        nsigma * sigma), to = (mu + nsigma * sigma), length.out = 1000), mean = mu, 
        sd = sigma)), c(mu + nsigma * sigma, 0)), border = NA, col = "salmon") + 
    abline(v = mu, col = "blue") + abline(h = 0) + abline(v = c(mu - nsigma * 
    sigma, mu + nsigma * sigma), col = "salmon"), mu = slider(-10, 10, initial = 0, 
    step = 0.25), sigma = slider(0.25, 4, initial = 1, step = 0.25), nsigma = slider(0, 
    4, initial = 0, step = 0.25))
```
```{r}
manipulate(plot(seq(from = (mu - 6 * sigma), to = (mu + 6 * sigma), length.out = 1000), 
    pnorm(seq(from = (mu - 6 * sigma), to = (mu + 6 * sigma), length.out = 1000), 
        mean = mu, sd = sigma), type = "l", xlim = c(-20, 20), xlab = "x", ylab = "f(x)", 
    main = "Cumulative Probability"), mu = slider(-10, 10, initial = 0, step = 0.25), 
    sigma = slider(0.25, 10, initial = 1, step = 0.25))  # plots the cumulative distribution function
```


---
title: "Module notebook"
output: html_notebook
---
# Module 9
```{r}
library(curl)
```

##Standard Errors and Confidence Intervals
To recap some of what we have covered in the last two modules, standard errors are key to calculating confidence intervals and to basic inferential statistics.

In Module 7, we calculated confidence intervals for one of our estimates of a population parameter (the population mean, an estimand), based on a sample statistic (the sample mean, our estimator). Let’s revist that process…

The general way to define a confidence interval based on data from a sample is as the value of the statistic being considered (e.g., the mean) ± some critical value ×× the standard error of the statistic.

The critical value is derived from the standardized version of a sampling distribution (e.g., the normal distribution) that corresponds the quantile limits we are interested in. For example, for the 95% CI around the mean, the critical value corresponds the range of quantiles above and below which we expect to see only 5% of the distribution of statistic values. This is equivalent to the ± 1 - (αα/2) quantiles, where αα=0.05, i.e., the ± 0.975 quantile that we have used before for calculating 95% CIs.

The standard error is the standard deviation of the sampling distribution, which, as noted above, is often estimated from the sample itself as σσ/sqrt(n)(n) but can also be calculated directly from the population standard deviation, if that is known.

Recall that in Module 8, we created a vector, v, containing 1000 random numbers selected from a normal distribution with mean 3.5 and standard deviation 4. We then calculated the mean, standard deviation, and standard error of the mean (SEM) based on a sample of 30 observations drawn from that vector, and we used the normal distribution to characterize the quantiles associated with the central 95% of the distribution to define the upper and lower bounds of the 95% CI.

```{r}
n <- 1000
mu <- 3.5
sigma <- 4
v <- rnorm(n, mu, sigma)
s <- sample(v, size = 30, replace = FALSE)
m <- mean(s)
m
```

```{r}
sd <- sd(s)
sd
sem <- sd(s)/sqrt(length(s))
sem
```
```{r}
lower <- m - qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci <- c(lower, upper)
ci
```

##The Central Limit Theorem

Thus far, our construction of CIs has implicitly taken advantage of one of the most important theorems in statistics, the Central Limit Theorem. The key importance of the CLT for us is that it states that the distribution of averages (or sums or other summary statistics…) of iid (independent and identically distributed) random variables becomes normal as the sample size increases. It is this fact that allows us to have a good sense of the mean and distribution of average events in a population even though we only observe one set of events and do not know what actual population distribution is. In fact, the CLT says nothing about the probability distribution for events in the original population, and that is exactly where its usefulness lies… that original probability distribution can be normal, skewed, all kinds of odd!

But we can nonetheless assume normality for the distribution of sample mean (or of the sum or mode, etc…) no matter what kind of probability distribution characterizes the initial population, as long as our sample size is large enough and our samples are independent. It is thus the CLT that allows us to make inferences about a population based on a sample.

Just to explore this a bit, let’s do some simulations. We are going to take lots of averages of samples from a particular non-normal distribution and then look at the distribution of those averages. Imagine we have some event that occurs in a population according to some probability mass function like the Poisson where we know λλ=14. Recall, then, that the expectations of μμ and σ2σ2 for the Poisson distribution are both=λλ.

Now let’s imagine taking a bunch of samples of size 10 from this population. We will take 1000 random samples of this size, calculate the average of each sample, and plot a histogram of those averages… it will be close to normal, and the standard deviation of the those average - i.e., of the sampling distribution - should be roughly equal to the estimated standard error, the square root of (λ/n)(λ/n). [Recall that λ is the expected variance, so this is simply the square root of (expected variance / sample size)]

```{r}
lambda <- 14
n <- 10
pop_se <- sqrt(lambda/n)  # the estimated SE
pop_se
```

```{r}
x <- NULL
for (i in 1:1000) {
    x[i] <- mean(rpois(n = n, lambda = lambda))
}
hist(x, breaks = seq(from = lambda - 4 * sqrt(lambda)/sqrt(n), to = lambda + 
    4 * sqrt(lambda)/sqrt(n), length.out = 20), probability = TRUE)
```

```{r}
sd <- sd(x)  # st dev of the sampling distribution
sd
```

```{r}
qqnorm(x)
qqline(x)
```

do this for samples of size 100, too. We see that the mean stays the same, the distribution is still normal, but the standard deviation - the spread - of the sampling distribution is lower.

```{r}
n <- 100
pop_se <- sqrt(lambda/n)  # the estimated SE
pop_se
```

```{r}
x <- NULL
for (i in 1:1000) {
    x[i] <- mean(rpois(n = n, lambda = lambda))
}
hist(x, breaks = seq(from = lambda - 4 * sqrt(lambda)/sqrt(n), to = lambda + 
    4 * sqrt(lambda)/sqrt(n), length.out = 20), probability = TRUE)
```

```{r}
sd <- sd(x)  # st dev of the sampling distribution
sd
```

```{r}
qqnorm(x)
qqline(x)
```
We can convert these distributions to standard normals by subtracting off the expected population mean (λ) and dividing by the standard error of the mean (an estimate of the standard deviation of the sampling distribution) and then plotting a histogram of those values along with a normal curve.
```{r}
curve(dnorm(x, 0, 1), -4, 4, ylim = c(0, 0.8))
z <- (x - lambda)/pop_se
hist(z, breaks = seq(from = -4, to = 4, length.out = 20), probability = TRUE, 
    add = TRUE)
```
Pretty normal looking, right?

Here’s an example of the CLT in action using sum() instead of mean()…
```{r}
n <- 100
x <- NULL
for (i in 1:1000) {
    x[i] <- sum(rpois(n = n, lambda = lambda))
}
hist(x, breaks = seq(min(x), max(x), length.out = 20), probability = TRUE)
```
Again, pretty normal looking!

###Take Home Points:

[1] The CLT states that, regardless of the underlying distribution, the distribution of averages (or sums or standard deviations, etc…) based on a large number of independent, identically distributed variables:

will be approximately normal,
will be centered at the population mean, and – will have a standard deviation roughly equal to the standard error of the mean.


Additionally, it suggests that variables that are expected to be the sum of multiple independent processes (e.g., measurement errors) will also have distributions that are nearly normal.

[2] Taking the mean and adding and subtracting the relevant standard normal quantile ×× the standard error yields a confidence interval for the mean, which gets wider as the coverage increases and gets narrower with less variability or larger sample sizes.

[3] As sample size increases, the standard error of the mean decreases and the distribution becomes more and more normal (i.e., has less skew and kurtosis, which are higher order moments of the distribution).

For a nice interactive simulation demonstrating the Central Limit Theorem, check out this cool website.

##Confidence Intervals for Sample Proportions

So far, we’ve talked about CIs for sample means, but what about for other statistics, e.g., sample proportions for discrete binary variables. For example, if you have a sample of n trials where you record the success or failure of a binary event, you obtain an estimate of the proportion of successes, x/nx/n. If you perform another n trials, the new estimate will vary in the same way that averages of a continuous random variable (e.g., zombie age) will vary from sample to sample. Taking a similar approach as above, we can generate confidence intervals for variability in the proportion of successes across trials.

Recall from our discussion of discrete random binary variables that the expectation for proportion of successes, which we will denote as ππ (where ππ, for “proportion”, is analogous to μμ, for “mean”) is simply the average number of successes across multiple trials.

The expected sampling distribution for the proportion of successes is approximately normal centered at ππ and *its standard deviation is estimated by sqrt(π(1−π)/n)π(1−π)/n), which is, essentially, the standard error of the mean*: it is the square root of (the expected variance / sample size). As above for μμ, if we do not already have a population estimate for ππ, we can estimate this from a sample as  phat= x/n

Note: *this expectation based on an approximation of the normal holds true only if both n×π and n×(1−π) are greater than roughly 5*, so we should check this when working with proportion data.

CHALLENGE:

Suppose a polling group in the Massachusetts is interested in the proportion of voting-age citizens in their state that already know they will vote for Elizabeth Warren in the upcoming November 6, 2018 midterm elections (don’t forget to vote!). The group obtains a yes or no answer from 1000 suitable randomly selected individuals. Of these individuals, 856 say they know they’ll vote for Senator Warren. How would we characterize the mean and variability associated with this proportion?
```{r}
n <- 1000
x <- 856
phat <- x/n  # our estimate of pi
phat
```
Are n×πn×π and n×(1−π)n×(1−π) both > 5? Yes!
```{r}
n * phat
n * (1 - phat)
```

```{r}
pop_se <- sqrt((phat) * (1 - phat)/n)
```
So, what is the 95% CI around our estimate of the proportion of people who already know how they will vote? **where does the 4 come from?**
```{r}
curve(dnorm(x, mean = phat, sd = pop_se), phat - 4 * pop_se, phat + 4 * pop_se)
upper <- phat + qnorm(0.975) * pop_se
lower <- phat - qnorm(0.975) * pop_se
ci <- c(lower, upper)
polygon(cbind(c(ci[1], seq(from = ci[1], to = ci[2], length.out = 1000), ci[2]), 
    c(0, dnorm(seq(from = ci[1], to = ci[2], length.out = 1000), mean = phat, 
        sd = pop_se), 0)), border = "black", col = "gray")
abline(v = ci)
abline(h = 0)
```
##Small Sample Confidence Intervals

Thus far, we have discussed creating a confidence interval based on the CLT and the normal distribution, and our intervals took the form:

mean ± Z (the quantile from the standard normal curve) × standard error of the mean

But, when the size of our sample is small (n < 30), instead of using the normal distribution to calculate our CIs, statisticians typically use a different distribution to generate the relevant quantiles to multiply the standard error by… the t distribution (a.k.a., Gosset’s t or Student’s t distribution).

Note that this is the typical case that we will encounter, as we often do not have information about the population that a sample is drawn from.

The t distribution is a continuous probability distribution very similar in shape to the normal is generally used when dealing with statistics (such as means and standard deviations) that are estimated from a sample rather than known population parameters. Any particular t distribution looks a lot like a normal distribution in that it is bell-shaped, symmetric, unimodal, and (if standardized) zero-centered.

The choice of the appropriate t distribution to use in any particular statistical test is based on the number of degrees of freedom (df), i.e., the number of individual components in the calculation of a given statistic (such as the standard deviation) that are “free to change”.

We can think of the t distribution as representing a family of curves that, as the number of degrees of freedom increases, approaches the normal curve. At low numbers of degrees of freedom, the tails of the distribution get fatter.

Confidence intervals based on the t distribution are of the form:

mean ± T (the quantile from the t distribution) × standard error of the mean

The only change from those based on the normal distribution is that we’ve replaced the Z quantile of the standard normal with a T quantile.

Let’s explore this a bit…

Recall that a standard normal distribution can be generated by normalizing our sample (subtracting the population mean from each observation and then dividing all of these differences by the population standard deviation)…

(mean(x)-μμ)/σσ

If we subtract the population mean from each observation but then divide each difference, instead, by the standard error of the mean, (mean(x)-μμ)/SEM, the result is not a normal distribution, but rather a t distribution! We are taking into account sample size by dividing by the standard error of the mean rather than the population standard deviation.

The code below plots a standard normal distribution and then t distributions with varying degrees of freedom, specified using the df= argument. As for other distributions, R implements density, cumulative probability,  quantile, and random functions for the t distribution.

**again, where is the 4 coming from?**
```{r}
mu <- 0
sigma <- 1
curve(dnorm(x, mu, 1), mu - 4 * sigma, mu + 4 * sigma, main = "Normal Curve=red\nStudent's t=blue", 
    xlab = "x", ylab = "f(x)", col = "red", lwd = 3)
for (i in c(1, 2, 3, 4, 5, 10, 20, 100)) {
    curve(dt(x, df = i), mu - 4 * sigma, mu + 4 * sigma, main = "T Curve", xlab = "x", 
        ylab = "f(x)", add = TRUE, col = "blue", lty = 5)
}
```
The fatter tails of the t distibution lead naturally to more extreme quantile values given a specific probability than we would see for the normal distribution. If we define a CI based on quantiles of the t distribution, they will be correspondingly slightly wider than those based on the normal distribution for small values of df.

We can see this as follows. Recall that above we estimated the 95% CI for a sample drawn from a normal distribution as follows:
```{r}
n <- 1e+05
mu <- 3.5
sigma <- 4
x <- rnorm(n, mu, sigma)
sample_size <- 30
s <- sample(x, size = sample_size, replace = FALSE)
m <- mean(s)
m
```

```{r}
sd <- sd(s)
sd
sem <- sd(s)/sqrt(length(s))
sem
```

```{r}
lower <- m - qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci_norm <- c(lower, upper)
ci_norm
```
Now, let’s look at the CIs calculated based using the t distribution for the same sample size. For sample size 30, the difference in the CIs is negligible.
```{r}
lower <- m - qt(1 - 0.05/2, df = sample_size - 1) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qt(1 - 0.05/2, df = sample_size - 1) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci_t <- c(lower, upper)
ci_t
```
However, if we use a sample size of 5, the CI based on the t distribution is much wider.
```{r}
sample_size <- 5
s <- sample(x, size = sample_size, replace = FALSE)
m <- mean(s)
m

sd <- sd(s)
sd

sem <- sd(s)/sqrt(length(s))
sem
```

```{r CI N dist small sample}
lower <- m - qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci_norm <- c(lower, upper)
ci_norm
```

```{r CI T dist small sample}
lower <- m - qt(1 - 0.05/2, df = sample_size - 1) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qt(1 - 0.05/2, df = sample_size - 1) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci_t <- c(lower, upper)
ci_t
```
# Module 10
```{r}

```

```{r}

```

```{r}

```

```{r}

```


